{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h8RJ9aonLmCt"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# necessary imports\n",
        "!pip install xmltodict\n",
        "import os\n",
        "import tarfile\n",
        "import xmltodict\n",
        "import pandas as pd\n",
        "import xml.etree.ElementTree as ET"
      ],
      "metadata": {
        "id": "kMAM9YWJL4YN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# extracting the xml records\n",
        "folder_path = \"drive/MyDrive/BAIS6100/Datasets/Project3_data\"\n",
        "\n",
        "# List all XML files in the folder\n",
        "xml_files = [f for f in os.listdir(folder_path) if f.endswith('.xml')]\n",
        "\n",
        "# Iterate through each XML file and save the text portion to a dataframe\n",
        "data_list = []\n",
        "\n",
        "for file_name in xml_files:\n",
        "    file_path = os.path.join(folder_path, file_name)\n",
        "\n",
        "    # Parse the XML file\n",
        "    tree = ET.parse(file_path)\n",
        "    root = tree.getroot()\n",
        "\n",
        "    # Extract 'TEXT' data\n",
        "    text_element = root.find('TEXT')\n",
        "    text_data = text_element.text if text_element is not None else \"No TEXT found\"\n",
        "\n",
        "    data_list.append({'File Name': file_name, 'Text': text_data})\n",
        "\n",
        "df = pd.DataFrame(data_list)"
      ],
      "metadata": {
        "id": "sjezPWuoL88u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Separate the dataframe into three columns, one for patientID (first part of file name), one for visit number (second part of file name), and one for the text\n",
        "df[['patientID', 'visit']] = df['File Name'].str.split(\"-\", n=1, expand=True)\n",
        "\n",
        "# Further split 'visit' into 'visit' and 'x' using \".\"\n",
        "df[['visit', 'x']] = df['visit'].str.split(\".\", n=1, expand=True)\n",
        "'\n",
        "df = df.rename(columns={'Text': 'text'})\n",
        "\n",
        "# Select relevant columns and sort by 'patientID'\n",
        "df = df[['patientID', 'visit', 'text']].sort_values(by=['patientID', 'visit'])"
      ],
      "metadata": {
        "id": "fbzYV2J-PpCC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.iloc[10, 2]) # view the text portion of the 10th record (example usage)"
      ],
      "metadata": {
        "id": "bkaAKSBMP3gn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# number of records\n",
        "print(len(df))"
      ],
      "metadata": {
        "id": "ZCOeFkLOXOa3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# number of patients\n",
        "print(len(df.groupby('patientID')))"
      ],
      "metadata": {
        "id": "_xDw9McsXUEi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Blood Pressure"
      ],
      "metadata": {
        "id": "N9NZqIze0844"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Extract systolic and diastolic blood pressure using regex\n",
        "df[['systole', 'diastole']] = df['text'].str.extract(r'(\\d{2,3})\\s*/\\s*(\\d{2,3})')\n",
        "\n",
        "# Convert extracted values to numeric\n",
        "df[['systole', 'diastole']] = df[['systole', 'diastole']].apply(pd.to_numeric, errors='coerce')\n",
        "\n",
        "# Drop rows with missing values\n",
        "df = df.dropna(subset=['systole', 'diastole'])\n",
        "\n",
        "# Categorize blood pressure levels\n",
        "def classify_bp(systole, diastole):\n",
        "    if systole > 180 or diastole > 120:\n",
        "        return \"Crisis\"\n",
        "    elif systole >= 140 or diastole >= 90:\n",
        "        return \"Hypertension 2\"\n",
        "    elif 130 <= systole <= 139 or 80 <= diastole <= 89:\n",
        "        return \"Hypertension 1\"\n",
        "    elif 120 <= systole <= 129 and diastole < 80:\n",
        "        return \"Elevated\"\n",
        "    elif systole < 120 and diastole < 80:\n",
        "        return \"Normal\"\n",
        "    return \"Unknown\"\n",
        "\n",
        "df.loc[:, 'level'] = df.apply(lambda row: classify_bp(row['systole'], row['diastole']), axis=1)\n",
        "\n",
        "# Remove \"Unknown\" values\n",
        "df = df[df['level'] != \"Unknown\"]\n",
        "\n"
      ],
      "metadata": {
        "id": "zcySTSy_Yx4F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.head)"
      ],
      "metadata": {
        "id": "gksMWADClJZm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save to csv\n",
        "#df.to_csv('drive/MyDrive/BAIS6100/project_df.csv', index=False)"
      ],
      "metadata": {
        "id": "jgo077aqFndU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot counts of each blood pressure level, faceted by visit number\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create factor for BP levels\n",
        "bp_levels = [\"Normal\", \"Elevated\", \"Hypertension 1\", \"Hypertension 2\", \"Crisis\"]\n",
        "df['level_fct'] = pd.Categorical(df['level'], categories=bp_levels, ordered=True)\n",
        "\n",
        "# Same for visits\n",
        "visit_labels = [\"Visit 1\", \"Visit 2\", \"Visit 3\", \"Visit 4\", \"Visit 5\"]\n",
        "df['visit_fct'] = pd.Categorical(df['visit'], categories=[\"01\", \"02\", \"03\", \"04\", \"05\"], ordered=True)\n",
        "\n",
        "# Faceted bar plot by visit number\n",
        "g = sns.catplot(\n",
        "    data=df,\n",
        "    x=\"level_fct\",\n",
        "    hue=\"level_fct\",\n",
        "    col=\"visit_fct\",  # Facet by visit\n",
        "    kind=\"count\",\n",
        "    palette=\"coolwarm\",\n",
        "    order=bp_levels,\n",
        "    col_wrap=3,\n",
        "    height=4,\n",
        "    aspect=1.2\n",
        ")\n",
        "\n",
        "g.set_titles(\"BP Distribution - {col_name}\")\n",
        "g.set_axis_labels(\"BP Level\", \"Num Observations\")\n",
        "g.set_xticklabels(rotation=45)\n",
        "g.set(ylim=(0, df['level_fct'].value_counts().max() + 5))\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Bg5fbHYblQia"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# or\n",
        "sns.countplot(data=df, x=\"visit\", hue=\"level\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "M0ha_nNwmI6b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Social History"
      ],
      "metadata": {
        "id": "QzB64WxLWrKs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "def extract_social_history(text):\n",
        "    # Find start index\n",
        "    if re.search(r'(?i)social history', text):\n",
        "        soc_startin = re.search(r'(?i)social history', text).start() + 15\n",
        "    elif 'SH' in text:\n",
        "        soc_startin = text.find('SH') + 3\n",
        "    else:\n",
        "        return None  # No recognizable start\n",
        "\n",
        "    rest_of_text = text[soc_startin:]\n",
        "\n",
        "    # Find end index: look for \"\\n\\n\\n\"\n",
        "    match = re.search(r'\\n\\n\\n', rest_of_text)\n",
        "    if match:\n",
        "        soc_endin = match.start()\n",
        "        soc_history = text[soc_startin:soc_startin + soc_endin]\n",
        "    else:\n",
        "        soc_history = text[soc_startin:soc_startin + 200]  # Default to 200 characters\n",
        "\n",
        "    return soc_history\n",
        "\n",
        "# Apply\n",
        "df['soc_history'] = df['text'].apply(extract_social_history)\n",
        "\n",
        "# Has social history by record\n",
        "df['has_history'] = df['soc_history'].notna()\n",
        "\n",
        "# Has social history by patient\n",
        "num_patients_with_history = df[df['has_history']]['patientID'].nunique()\n",
        "\n",
        "# Combine social history per patient for topic modeling\n",
        "combined_hist = (\n",
        "    df[df['has_history']]\n",
        "    .groupby('patientID')['soc_history']\n",
        "    .apply(lambda texts: ' '.join(texts))\n",
        "    .reset_index()\n",
        "    .rename(columns={'soc_history': 'history'})\n",
        ")\n"
      ],
      "metadata": {
        "id": "GnDgXDSG_hlb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "U7vBfWT__n3j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "combined_hist"
      ],
      "metadata": {
        "id": "dzbfTsD4__xi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Topic Modeling"
      ],
      "metadata": {
        "id": "E_IE7rk_wOYT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "import nltk\n",
        "nltk.download(\"stopwords\")\n",
        "stemmer = nltk.stem.SnowballStemmer(\"english\", ignore_stopwords=True)\n",
        "class StemmedCountVectorizer(CountVectorizer):\n",
        "  def build_analyzer(self):\n",
        "    analyzer = super(StemmedCountVectorizer, self).build_analyzer()\n",
        "    return lambda doc: ([stemmer.stem(w) for w in analyzer(doc)])"
      ],
      "metadata": {
        "id": "l-NlEYVcwRLD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Choose # of Topics"
      ],
      "metadata": {
        "id": "Irz008Hnxbin"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "df_train, df_test = train_test_split(combined_hist, test_size=0.33, random_state=2021)\n",
        "df_train.reset_index(drop=True, inplace=True)\n",
        "df_test.reset_index(drop=True, inplace=True)"
      ],
      "metadata": {
        "id": "WzvYIZIBxAHx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "global_stopwords = nltk.corpus.stopwords.words(\"english\")\n",
        "\n",
        "vectorizer = StemmedCountVectorizer(stop_words=global_stopwords,\n",
        "                                    max_features=100)\n",
        "train_x = vectorizer.fit_transform(df_train[\"history\"])\n",
        "test_x = vectorizer.transform(df_test[\"history\"])"
      ],
      "metadata": {
        "id": "F221raFhxkYF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lda = LatentDirichletAllocation(n_jobs=-1,\n",
        "                                random_state=6100)\n",
        "num_topics = [2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
        "perplexity = []\n",
        "for i in num_topics:\n",
        "  print(i)\n",
        "  lda.set_params(n_components=i)\n",
        "  lda.fit(train_x)\n",
        "  perplexity.append(lda.perplexity(test_x))\n",
        "\n",
        "plt.plot(num_topics, perplexity)\n",
        "plt.xlabel(\"Number of Topics\")\n",
        "plt.ylabel(\"Perplexity\")"
      ],
      "metadata": {
        "id": "AWFSg07Px3mk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "High perplexity values, doesn't mean much in a vacuum, but could suggest that there is high variance in the social histories."
      ],
      "metadata": {
        "id": "V63_0NY9yV2K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit LDA on whole corpus\n",
        "DTM = vectorizer.fit_transform(combined_hist[\"history\"])\n",
        "lda = LatentDirichletAllocation(n_components=3,\n",
        "                                n_jobs=-1,\n",
        "                                random_state=6100)\n",
        "lda.fit(DTM)"
      ],
      "metadata": {
        "id": "y9nkGh01y3Vs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Trying PyLDAvis"
      ],
      "metadata": {
        "id": "aFqgRljT0OK9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install pyLDAvis\n",
        "#!pip install numpy==1.24.4\n",
        "import pyLDAvis\n",
        "import pyLDAvis.lda_model\n",
        "\n",
        "html = pyLDAvis.lda_model.prepare(lda, DTM, vectorizer)\n",
        "\n",
        "# save html\n",
        "#pyLDAvis.save_html(html, 'drive/MyDrive/BAIS6100/lda_sklearn.html')"
      ],
      "metadata": {
        "id": "UIelphq30SCR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Within-topic and between-topic similarity"
      ],
      "metadata": {
        "id": "o-Cd4-wF4278"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.preprocessing import normalize\n",
        "\n",
        "# get document-topic distribution matrix (rows = docs, cols = topics)\n",
        "doc_topic_dist = lda.transform(DTM)  # shape: (num_docs, num_topics)\n",
        "print(doc_topic_dist.shape)\n",
        "# normalize for cosine similarity\n",
        "doc_topic_dist = normalize(doc_topic_dist, norm='l2')\n",
        "\n",
        "# Assign each document to its dominant topic\n",
        "doc_labels = np.argmax(doc_topic_dist, axis=1)\n",
        "\n",
        "# within-cluster similarities\n",
        "within_sims = []\n",
        "for topic_id in range(lda.n_components):\n",
        "    indices = np.where(doc_labels == topic_id)[0] # get indices for documents assigned to topic i\n",
        "    cluster_docs = doc_topic_dist[indices] # subset docs according to those indices\n",
        "    sims = cosine_similarity(cluster_docs) # computes cosine similarity for every pair of documents\n",
        "    upper_tri = sims[np.triu_indices_from(sims, k=1)] # matrix is necessarily symmetric, so take just the upper triangle\n",
        "    if len(upper_tri) > 0:\n",
        "        within_sims.append(np.mean(upper_tri))\n",
        "\n",
        "avg_within_similarity = np.mean(within_sims)\n",
        "\n",
        "# between-cluster similarity\n",
        "topic_centroids = []\n",
        "for topic_id in range(lda.n_components):\n",
        "    indices = np.where(doc_labels == topic_id)[0]\n",
        "    if len(indices) == 0:\n",
        "        continue\n",
        "    topic_centroids.append(doc_topic_dist[indices].mean(axis=0))\n",
        "\n",
        "centroid_sim_matrix = cosine_similarity(topic_centroids)\n",
        "between_sims = centroid_sim_matrix[np.triu_indices_from(centroid_sim_matrix, k=1)]\n",
        "avg_between_similarity = np.mean(between_sims)\n"
      ],
      "metadata": {
        "id": "3kfEcH3p47LV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(within_sims) # within-cluster similarity scores for each topic\n",
        "print(avg_within_similarity) # average similarity for the three topics\n",
        "print(between_sims) # average between-cluster simlarity for each topic with the other two\n",
        "print(avg_between_similarity) # average of the three"
      ],
      "metadata": {
        "id": "5bJuOT3w5FaU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}